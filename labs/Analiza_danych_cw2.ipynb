{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza danych – ćwiczenia 2.\n",
    "### Wstępne przetwarzanie danych\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autor: mgr inż. Bartosz Czech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rozdzielanie zestawu danych na podzbiory uczące i testowe\n",
    "Aby umożliwić komputerom proces uczenia, niezbędne jest właściwe przygotowanie danych. W uczeniu maszynowym, dane dzieli się zazwyczaj na dwa podzbiory – uczący i testowy. Dane uczące pozwala maszynie na poznanie wzorców zachodzących w zbiorze umożliwiających dokonanie predykcji/klasyfikacji. Zbiór testowy rozumiany jest jako ostateczny sprawdzian przygotowanego modelu. W przypadku danych niezbalansowanych (jedna z klas występuje bardzo rzadko), dobre przygotowanie danych uczących i testowych wymaga jeszcze bardziej precyzyjnego podziału danych. Do podziału danych, przydatna będzie funkcja [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem brakujących danych\n",
    "W eksperymentach data science, problem brakujących danych jest dosyć powszechny. Wynika on w głównej mierze z błędów w trakcie zbierania informacji lub braku możliwości pomiarów. Większość algorytmów nie potrafi sobie poradzić z brakami danych lub generuje fałszywe wyniki. Jedną z najprostszych metod przetwarzania takich danych, jest usunięcie obserwacji z brakującymi cechami. W tym celu pomocna może być funkcja [pandas.DataFrame.dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html). \n",
    "Usuwanie rekordów może być natomiast problematyczne, jeśli braki danych pojawiają się często. Tracimy wtedy cenne informacje. Brakujące dane mozemy uzupełnić dzięki technikom imputacji, wykorzystując do tego średnią, medianę lub dominantę."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizacja/standaryzacja zmiennych\n",
    "W celu poprawnego zbudowania modelu sztucznej sieci neuronowej dane wymagają wstępnego przetworzenia. Brak normalizacji zmiennej może być przyczyną spowolnienia i niestabilności procesu uczenia modelu, a także mogą decydować o większym wpływie zmiennej nieznormalizowanej na budowany model. Normalizacja polega na skalowaniu pierwotnych danych do danego, innego przedziału. Najczęściej wykorzystywanymi metodami transformacji danych jest metoda min-max lub standaryzacja $\\sim \\mathcal{N}(0,\\,1$) (nazywana również metodą z-score). Powyższa normalizacja jest stosowana dla zmiennych ilościowych.\n",
    "\n",
    "W przypadku zmiennych o charakterze jakościowym konieczne jest sprowadzenie wartości do postaci numerycznej, czyli dogodnej do przetworzenia przez sieć neuronową (np. przy zastosowaniu metody jeden-z-N, w której każdej wartości zmiennej przyporządkowuje się wektor wartości binarnych). W tym celu może być przydatna funkcja [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Utwórz 2 sekwencje 3-merowe (BEHIND i BEFORE) na podstawie informacji o pojedynczych nukleotydach.\n",
    "2. Dokonaj podziału danych na zbiory uczące i testowe.\n",
    "3. Sprawdź kompletność danych. W przypadku rekordów z brakującą zmienną zdecyduj o ich usunięciu lub dokonaj imputacji brakującej zmiennej (średnia, dominanta, mediana, interpolacja modelem liniowym).\n",
    "4. Dokonaj normalizacji lub standaryzacji zmiennych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as n\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/logreg.txt\", sep = \";\", dtype={'genotype': np.object})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
